# -*- coding: utf-8 -*-
"""Task3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Blh_CvIf39MyQD0sKMKQNKVQvy-ipsOT
"""

!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install pyspark

# Installer Java
!apt-get install openjdk-11-jdk-headless -qq > /dev/null

# Télécharger Spark depuis l'archive Apache
!wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz

# Extraire Spark
!tar -xvzf spark-3.4.1-bin-hadoop3.tgz

# Installer PySpark
!pip install -q pyspark

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("WikipediaArticlesAnalysis") \
    .getOrCreate()

from google.colab import files
uploaded = files.upload()

# Read the lines from wiki.txt
lines = spark.read.text("wiki.txt").rdd.map(lambda r: r[0])

# Properly split into url, title, and text
def split_line(line):
    parts = line.split('\t')
    url = parts[0] if len(parts) > 0 else ""
    title = parts[1] if len(parts) > 1 else ""
    text = "\t".join(parts[2:]) if len(parts) > 2 else ""
    return (url, title, text)

# Apply the correct splitting function
articles = lines.map(split_line)

# Create the DataFrame
articles_df = articles.toDF(["url", "title", "text"])

# Show the first 5 rows
articles_df.show(5, truncate=False)

from pyspark.sql.functions import explode, split, length, col

# 1. Split the text into words
words_df = articles_df.select(
    explode(
        split(col("text"), r"\s+")
    ).alias("word")
)

# 2. Remove empty strings
words_df = words_df.filter(col("word") != "")

# 3. Calculate the length of each word and find the longest one
longest_word = words_df.withColumn(
    "word_length", length(col("word"))
).orderBy(
    col("word_length").desc()
).limit(1)

# 4. Show the longest word
longest_word.show(truncate=False)

from pyspark.sql.functions import avg

# 1. Calculate the average word length
average_word_length = words_df.withColumn(
    "word_length", length(col("word"))
).select(
    avg("word_length").alias("average_word_length")
)

# 2. Show the result
average_word_length.show()

from pyspark.sql.functions import regexp_extract, lower, count

# 1. Keep only Latin words (letters A-Z or a-z)
latin_words_df = words_df.withColumn(
    "latin_word",
    regexp_extract(col("word"), r"\b([A-Za-z]+)\b", 1)
).filter(
    col("latin_word") != ""
)

# 2. Count frequency of each Latin word
latin_word_counts = latin_words_df.groupBy(
    lower(col("latin_word")).alias("word")
).agg(
    count("*").alias("count")
)

# 3. Find the most frequent Latin word
most_frequent_latin_word = latin_word_counts.orderBy(
    col("count").desc()
).limit(1)

# 4. Show the result
most_frequent_latin_word.show(truncate=False)

from pyspark.sql.functions import when, sum as _sum

# 1. Create a new column to check if the word starts with an uppercase letter
words_with_capital = words_df.withColumn(
    "starts_with_capital",
    when(
        col("word").rlike(r"^[A-ZА-Я]"),  # A-Z (anglais) + А-Я (russe)
        1
    ).otherwise(0)
)

# 2. Group by the lowercase version of the word
word_capital_counts = words_with_capital.groupBy(
    lower(col("word")).alias("word")
).agg(
    count("*").alias("total_count"),
    _sum("starts_with_capital").alias("capital_count")
)

# 3. Filter: more than 10 occurrences and more than 50% starting with capital
final_words = word_capital_counts.filter(
    (col("total_count") > 10) &
    (col("capital_count") / col("total_count") > 0.5)
)

# 4. Show the result
final_words.show(truncate=False)

# 1. Find short abbreviations (2-3 letters + dot)
short_abbreviations = words_df.filter(
    col("word").rlike(r"^\w{2,3}\.$")
)

# 2. Count occurrences
short_abbreviations_counts = short_abbreviations.groupBy(
    col("word")
).agg(
    count("*").alias("count")
)

# 3. Show results
short_abbreviations_counts.orderBy(col("count").desc()).show(truncate=False)

# 1. Find long abbreviations (at least one internal dot)
long_abbreviations = words_df.filter(
    col("word").rlike(r"^\w+\.\w+.*\.$")
)

# 2. Count occurrences
long_abbreviations_counts = long_abbreviations.groupBy(
    col("word")
).agg(
    count("*").alias("count")
)

# 3. Show results
long_abbreviations_counts.orderBy(col("count").desc()).show(truncate=False)